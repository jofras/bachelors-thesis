{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# performance evaluation\n",
    "\n",
    "this notebook is all about performance evaluation, i.e. how well do these models do on practical tasks like word analogies and similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "import urllib.request\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first, load whatever models you want to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../../../embedding_gen/embeddings/w2v_pfv_100_5_f_5/model'\n",
    "full_path = os.path.abspath(model_path)\n",
    "model = Word2Vec.load(full_path)\n",
    "wordvecs = model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word similarity and analogy\n",
    "\n",
    "to evaluate how well the model captures semantic and syntactic relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### google analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44464203763194127\n"
     ]
    }
   ],
   "source": [
    "analogy_url = \"https://raw.githubusercontent.com/piskvorky/gensim/refs/heads/develop/gensim/test/test_data/questions-words.txt\"\n",
    "analogy_path = os.path.join(\"eval\", \"questions-words.txt\")\n",
    "\n",
    "if not os.path.exists(analogy_path):\n",
    "    urllib.request.urlretrieve(analogy_url, analogy_path)\n",
    "\n",
    "analogy_res = wordvecs.evaluate_word_analogies(analogy_path)\n",
    "print(analogy_res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wordsim-353"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(PearsonRResult(statistic=0.6460058078485802, pvalue=4.4623722878998506e-43), SignificanceResult(statistic=0.6682090256703265, pvalue=5.238914856995817e-47), 0.0)\n"
     ]
    }
   ],
   "source": [
    "ws_url = \"https://raw.githubusercontent.com/piskvorky/gensim/refs/heads/develop/gensim/test/test_data/wordsim353.tsv\"\n",
    "ws_path = os.path.join(\"eval\", \"wordsim353.txt\")\n",
    "\n",
    "if not os.path.exists(ws_path):\n",
    "    urllib.request.urlretrieve(ws_url, ws_path)\n",
    "\n",
    "ws_res = wordvecs.evaluate_word_pairs(ws_path)\n",
    "print(ws_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simlex-999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(PearsonRResult(statistic=0.3768184128980168, pvalue=4.6683703044622626e-35), SignificanceResult(statistic=0.35686805603251986, pvalue=2.262765902862798e-31), 0.0)\n"
     ]
    }
   ],
   "source": [
    "simlex_url = \"https://raw.githubusercontent.com/piskvorky/gensim/refs/heads/develop/gensim/test/test_data/simlex999.txt\"\n",
    "simlex_path = os.path.join(\"eval\", \"simlex999.txt\")\n",
    "\n",
    "if not os.path.exists(simlex_path):\n",
    "    urllib.request.urlretrieve(simlex_url, simlex_path)\n",
    "\n",
    "simlex_res = wordvecs.evaluate_word_pairs(simlex_path)\n",
    "print(simlex_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MEN dataset\n",
    "\n",
    "here some preprocessing is needed to get the scores into the right range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized scores saved to eval/men_normalized.txt\n"
     ]
    }
   ],
   "source": [
    "input_file = \"eval/men.txt\"  \n",
    "output_file = \"eval/men_normalized.txt\"\n",
    "\n",
    "with open(input_file, \"r\") as infile, open(output_file, \"w\") as outfile:\n",
    "    for line in infile:\n",
    "        parts = line.rsplit(\" \", 1) \n",
    "        if len(parts) == 2:\n",
    "            word_pair, score = parts[0], float(parts[1])\n",
    "            normalized_score = (score * 10) / 50  # normalize to [0,10]\n",
    "            words = word_pair.split()\n",
    "            outfile.write(f\"{words[0]}\\t{words[1]}\\t{normalized_score:.2f}\\n\")  \n",
    "\n",
    "print(f\"normalized scores saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(PearsonRResult(statistic=0.7007690670824924, pvalue=0.0), SignificanceResult(statistic=0.7073827886513595, pvalue=0.0), 0.0)\n"
     ]
    }
   ],
   "source": [
    "men_path = \"eval/men_normalized.txt\"\n",
    "men_res = wordvecs.evaluate_word_pairs(men_path)\n",
    "print(men_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
